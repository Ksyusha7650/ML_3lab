#Лабораторная работа №3

**Тема**: Генерация текста в стиле творчества Пушкина  

**Цель**: Разработка и реализация рекуррентной нейронной сети с архитектурой LSTM на уровне символов для задачи генерации текста в в стиле выбранного исполнителя. 

**Исполнители**:  
- Куляев Е.В.  
- Рухлова К.А.  
## Содержание

- [Теоретическая часть](#теоретическая-часть)
- [Описание разработанной системы](#описание-разработанной-системы-алгоритмы-принципы-работы-архитектура)
- [Результаты](#результаты)
- [Выводы](#выводы)
- [Список использованных источников](#список-использованных-источников)

## Теоретическая часть
### Задача генерации текста

Генерация текста — одна из ключевых задач обработки естественного языка (NLP), направленная на создание логически последовательного, грамматически правильного и семантически осмысленного текста. Она находит применение в различных областях: автоматический перевод, чат-боты, генерация резюме текстов, креативное письмо, программирование и др.
В задаче генерации текста модель получает на вход некоторый контекст (например, начало предложения, набор ключевых слов или вопрос) и должна выдать продолжение текста, которое максимально соответствует семантике и синтаксису входных данных.
Современные методы генерации текста используют архитектуры на основе трансформеров, такие как GPT, BART, T5 и другие. Эти модели обладают способностью захватывать длинные зависимости в тексте и учитывать широкий контекст, что существенно повышает качество генерируемого контента.

### Архитектура LSTM на уровне символов: принципы генерации текста и особенности
Архитектура LSTM (Long Short–Term Memory) — это разновидность рекуррентных нейронных сетей, специально спроектированная для работы с последовательными данными и способная эффективно сохранять долгосрочную информацию. В контексте генерации текста на уровне символов она моделирует каждый символ как элемент последовательности и предсказывает следующий символ на основе всего предыдущего контекста.

## Ключевые характеристики

| Характеристика             | Описание                                                                                     |
| :------------------------- | :------------------------------------------------------------------------------------------- |
| **Ячейка памяти**   | Хранит и обновляет внутреннее состояние, отвечающее за долгосрочные зависимости.       |
| **Скрытое состояние**  | Выход ячейки, несущий краткосрочный контекст для предсказания текущего шага.                 |
| **Входной gate**    | Решает, какую часть новой информации добавить в память.                                      |
| **Забывающий gate** | Контролирует, какую часть старого состояния стереть (“забыть”).                              |
| **Выходной gate**   | Определяет, какую часть памяти отдать на выход в виде скрытого состояния.                    |
| **Градиентный контроль**   | Благодаря архитектуре ворот LSTM устойчив к взрывным/затухающим градиентам при BPTT.         |
| **Обработка символов**     | Каждый символ кодируется как one‑hot или через слой эмбеддингов. |
| **Stacked LSTM**           | Возможность «глубокой» сети из нескольких LSTM‑слоёв для большей выразительности.            |
| **Регуляризация**          | Dropout между слоями и регулировка размера скрытого состояния.                               |


## Принципы генерации текста

1. **Подготовка стартовой последовательности**:
   - Выбирается «seed» — начальный набор символов (например, слово или фраза).
   - Каждый символ этого «seed» последовательно превращается в векторное представление (one‑hot или через слой эмбеддингов).
   - Внутренние состояния сети (память и скрытое состояние) инициализируются нулями или малыми случайными значениями.

2. **Поэтапная обработка символов**:
   - На каждом шаге сеть получает вектор текущего символа и своё предыдущее состояние.
   - Внутри LSTM специальные «ворота» решают, какую информацию сохранить в памяти, какую — забыть, и какую — вывести наружу.
   - В результате обновляются внутреннее «долговременное» состояние и «кратковременный» выход, отражающие весь контекст до этого момента.

3. **Получение распределения вероятностей**:
   - Кратковременный выход преобразуется в набор чисел (логитов), по одному на каждый символ словаря.
   - С помощью функции softmax эти числа превращаются в вероятности—показывают, насколько каждый возможный следующий символ согласуется с контекстом.

4. **Выбор следующего символа**:
   - Жадный выбор: берётся символ с максимальной вероятностью.
   - Сэмплирование: выбирается случайный символ согласно распределению вероятностей; при этом используются механизмы «температуры», чтобы сделать распределение более острой (чёткий выбор) или более плоским (более разнообразный выбор).

5. **Итерация цикла**:
   - Сгенерированный символ снова превращается в вектор и подаётся на вход в следующий шаг.
   - Сеть обновляет свои состояния и выдаёт новый набор вероятностей для следующего символа.
   - Процесс повторяется до тех пор, пока не будет достигнута заданная длина текста или не встретится специальный признак остановки.

6. **Сборка и пост‑обработка**:
   - Все сгенерированные символы собираются в итоговую строку.
   - При необходимости выполняется коррекция регистра, разбиение на слова или форматирование (например, добавление пробелов после знаков препинания).

### Ключевые особенности LSTM-модели на уровне символов

- **Ворота памяти**: обеспечивают устойчивость к взрывным/затухающим градиентам.**  
- **Долговременная память**: сохраняет контекст на произвольно больших расстояниях. 
- **Stacked LSTM**: легко углубляется для более сложных зависимостей.
- **Регуляризация**: dropout между слоями и раннее останавливание. 
- **Температура сэмплирования**: управляет балансом между точностью и разнообразием. 
- **Gradient clipping**: предотвращает взрыв градиентов.
- **Эмбеддинги символов**: компактное представление вместо one‑hot.
- **Лёгкая донастройка**: быстро адаптируется к новым текстам.

### Метрики оценки качества генерации текста:
Оценка качества сгенерированных текстов является нетривиальной задачей, поскольку она должна учитывать как грамматическую правильность, так и смысловую релевантность. Существует ряд метрик, как автоматических, так и основанных на человеческой оценке:

- **chrF(++)** — символьная метрика, основанная на совпадении n-грамм, подходит для морфологически богатых языков.
- **Perplexity** — измеряет, насколько хорошо языковая модель предсказывает следующий токен. Чем ниже perplexity, тем лучше модель.
- **Distinct-n**  — измеряет разнообразие генерируемых текстов, основываясь на количестве уникальных n-грамм.
- **Novelty** — показывает, насколько новый текст отличается от обучающего корпуса, важен для генерации креативных текстов.
- **LLM-as-judge** — сравнительно новый подход, при котором большая языковая модель (например, GPT-4) используется как оценщик качества других моделей на основе логических, семантических и стилистических критериев.

---
## Описание разработанной системы (алгоритмы, принципы работы, архитектура)
Разработана нейросетевая модель на базе LSTM, которая обучается предсказывать следующий символ в стихотворном тексте на основе предыдущей последовательности длиной 100 символов. При генерации текст «проскальзывает» через вложенный слой эмбеддингов, двухслойную LSTM с размером скрытого состояния 384 и линейный слой на число символов словаря.
### Алгоритм работы

#### 1. На вход подаётся стартовая строка, каждый символ которой кодируется в индекс.
#### 2. Инициализируется скрытое состояние LSTM нулями.
#### 3. Для каждого шага модель предсказывает распределение вероятностей по всему словарю, из которого семплируется следующий символ с учётом заданной «температуры» (коэффициент сглаживания).
#### 4. Прогон продолжается до достижения заданной длины вывода.

После обучения система предлагает два режима работы:
- **Интерактивная генерация**  
  Модель генерирует продолжение по начальной строке

- **Пакетная оценка**  
  Автоматическая генерация текстов для тестовой выборки с расчетом метрик качества

**Параметры генерации**:
| Описание                                                                              | Значение по умолчанию |
| ------------------------------------------------------------------------------------- | --------------------- |
| Коэффициент «жёсткости» распределения (низкая—консервативно, высокая—более креативно) | 0.4                   |
| Размерность эмбеддингов символов                                                      | 128                   |
| Число нейронов в скрытом слое LSTM                                                    | 384                   |
| Число слоёв LSTM                                                                      | 2                     |


#### 4. Оценка качества

**Ключевые метрики**:
- `chrF++` - оценка качества генерации
- `Distinct-1/2` - разнообразие слов/биграмм
- `Novelty` - % новых слов
- `Perplexity` - уверенность модели

---
## Результаты
В результате выполнения работы была реализована двухслойная LSTM-модель для генерации поэтических строк в стиле Пушкина.
Основные результаты:
Модель успешно обучена на собранном датасете, демонстрируя способность генерировать тексты, стилистически близкие к оригинальным произведениям исполнителя.

Качество генерации подтверждено метриками chrF(++), Perplexity, Distinct-n и Novelty, а также экспертной оценкой через LLM-as-judge.
### Динамика метрик в процессе обучения

**Сравнительный анализ по эпохам:**

| Метрика       | 5 эпох       | Интерпретация |
|---------------|--------------|---------------|
| **chrF++**    | 18.9        |  Текст близок по структуре и лексике, но не копируется |
| **Distinct-1**| 0.119        | Хорошее разнообразие на уровне отдельных символов |
| **Distinct-2**| 0.596        | Модель редко застревает на повторениях одних и тех же сочетаний |
| **Novelty %** | 4.92        | Текст близок к стилю оригинала |
| **Perplexity**| 1.30         | Модель высоко уверена в своих предсказаниях |

### Визуализация прогресса

![Screenshot_2](https://github.com/user-attachments/assets/039bef17-b595-4eb0-b34d-f8e4a5dd010e)


Рисунок 1 - Примеры сгенерированных текстов

## Выводы

На основе проведённого эксперимента с LSTM-моделью для поэтической генерации можно сделать следующие выводы:

1. **Эффективность обучения**:
- Качество структурно‑лексического соответствия корпусу высокое (chrF++ = 18.9).
- Модель демонстрирует уверенность в предсказаниях (перплексия всего 1.30), что указывает на хорошую сходимость за 5 эпох.

2. **Особенности генерации**:
- Сохраняется стилистическая близость к обучающему тексту (Novelty ≈ 4.9%), при этом уникальные элементы не копируются дословно.
- Генерация отличается хорошим разнообразием: Distinct‑1 = 0.119 и Distinct‑2 = 0.596, что говорит об отсутствии чрезмерных повторов биграмм.

Эксперимент подтвердил, что выбранная архитектура LSTM за пять эпох эффективно адаптируется к поэтическому стилю корпуса, обеспечивая баланс между узнаваемостью и креативностью.
## Список использованных источников

1. Habr : сайт - URL: https://habr.com/ru/companies/wunderfund/articles/331310/ (дата обращения: 08.05.2025).
2. Habr : сайт - URL: https://habr.com/ru/articles/873332/ (дата обращения: 10.05.2025).
4. Васильев, А. М. Искусственный интеллект и обработка естественного языка : учебное пособие / А. М. Васильев. – Москва : КНОРУС, 2022. – 304 с. – ISBN 978-5-406-09581-2.
